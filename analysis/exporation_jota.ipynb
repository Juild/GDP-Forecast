{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # append the directory above where the databese is.\n",
    "from utils import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "#import xgboost as xgb\n",
    "from numba import jit\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "%load_ext line_profiler\n",
    "### disable scientific notation in pandas\n",
    "pd.set_option('display.float_format', '{:.2f}'.format) ### display up to 2 decimal pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine four years in one row and create the new dataframe, still experimental takes aprox 3 minutes\n",
    "def combine_years(df):\n",
    "    row_aux = pd.Series()\n",
    "    row_final = pd.Series()\n",
    "    dfs = []\n",
    "    n = 1\n",
    "    d = {column:column for column in df.columns}\n",
    "    for index in df.index:\n",
    "        row_aux = df.loc[index].rename({column: f\"{column}.year.{index[1]}\" for column in df}) # rename all columns\n",
    "        row_final = row_final.append(row_aux)\n",
    "        if np.mod(n, 4) == 0:\n",
    "            dfs.append(row_final.to_frame().T)\n",
    "            row_aux = pd.Series()\n",
    "            row_final = pd.Series()\n",
    "        n += 1\n",
    "    return pd.concat(dfs)\n",
    "%lprun -f  combine_years df_final = combine_years(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_others_GDP(df_origin):\n",
    "    df = df_origin[config.GDP].copy()\n",
    "    n_countries = df.index.get_level_values(level=\"CountryCode\").nunique()\n",
    "    countries = df.index.get_level_values(level=\"CountryCode\").unique()\n",
    "    n_years = df.index.get_level_values(level=\"Year\").nunique()\n",
    "    n_rows = df.shape[0]\n",
    "    dfs = []\n",
    "    m = 0\n",
    "    while(m + n_years <= n_rows):\n",
    "        df_slice = df.iloc[m:m + n_years]\n",
    "        dfs.append(df_slice)\n",
    "        m += n_years\n",
    "    # actually dfs elements are series, we will keep working with them because I've seen it is lighter to work with\n",
    "    # series than with dataframes\n",
    "    for i in range(len(dfs)):\n",
    "        df_aux = dfs[i]\n",
    "        df_copies = []\n",
    "        for j in range(n_countries):\n",
    "            df_copies.append(dfs[i].copy())\n",
    "        dfs[i] = pd.concat(df_copies)\n",
    "    i = 0\n",
    "    for series in dfs:\n",
    "        series.rename(f\"{config.GDP}_{countries[i]}\", inplace=True)\n",
    "        i += 1\n",
    "    dfs_frames = [df.to_frame().reset_index(drop=True) for df in dfs]\n",
    "    dfs_joined = dfs_frames[0].join(dfs_frames[1:], how=\"left\")\n",
    "    dfs_joined.set_index(df.index, inplace=True)\n",
    "    df_final = df.join(dfs_joined)\n",
    "    return df_final.drop(config.GDP) # now we don't want the original gdp as it's repeated in the column with key f\"config.GDP_{country}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(config.DATABASE_PATH) as connection:\n",
    "    df = pd.read_sql(\"SELECT * FROM CountryIndicators\", connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rm_countries_no_GDP(df):\n",
    "    countries_gdp = df[df[\"IndicatorCode\"] == config.GDP][\"CountryCode\"].to_list() #countries with gdp\n",
    "    countries = df[\"CountryCode\"].unique()\n",
    "    countries_no_gdp = list(set(sorted(countries)) - set(sorted(countries_gdp))) # set difference =  countries with no gdp\n",
    "    return df.drop(countries_no_gdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivoted = df.pivot(index=[\"CountryCode\",\"Year\"], columns=\"IndicatorCode\", values=\"Value\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = rm_countries_no_GDP(df_pivoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_features = add_others_GDP(df_cleaned) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = df_pivoted.pop(config.GDP)\n",
    "df_features.drop(index=2010, level=\"Year\", inplace=True)\n",
    "df_target.drop(index=1960, level=\"Year\", inplace=True)\n",
    "# up until here we have the basic features selected. Now we want to add more features, like the GDP of each country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_features\n",
    "y = df_target\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "imp_mode = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "# genereate imputed dataframes\n",
    "X_imp = imp_mode.fit_transform(X) \n",
    "y_imp = imp_mode.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imp, y_imp, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_hyperparams = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.1,\n",
    "    'loss': 'ls'\n",
    "}\n",
    "gbm_model = GradientBoostingRegressor(**gbm_hyperparams)\n",
    "gbm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "y_pred = gbm_model.predict(X_test)\n",
    "rmse = np.sqrt(np.mean((y_pred - y_test)**2))\n",
    "print(f\"RMSE = {rmse} \\n R2 = {r2_score(y_test, y_pred)}\")\n"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.set_index(df_cleaned.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.join(df_final)"
   ]
  }
 ]
}